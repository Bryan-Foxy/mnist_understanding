{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**\n",
    "\n",
    "This notebook was created on *April 23, 2024, at 19:08*  with the intention of providing detailed explanations and interpretations of the processes involved. While Python scripts are powerful, they can sometimes be complex and require a significant investment in terms of analysis and understanding. This document aims to simplify this task by breaking down and explaining each code segment in detail.\n",
    "\n",
    "In this notebook, we will undertake to revisit and detail the implementations of the MNIST dataset, a dataset widely used in the machine learning community, initially popularized by Yann LeCun. The goal is to implement two types of neural architectures: a Multi-Layer Perceptron (MLP) and a Convolutional Neural Network (CNN). We will then compare their performances through various metrics.\n",
    "\n",
    "Additionally, special attention will be given to visualizing backpropagation to identify the global optimum. This approach aims to illustrate not only how modifications to the weights affect model accuracy during training but also to demonstrate optimization dynamics in action.\n",
    "\n",
    "This work is essential for those looking to deepen their understanding of deep learning fundamentals as well as for those wishing to refine their ability to optimize machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data are ready!\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_data = torchvision.datasets.MNIST(root='data', train=True,\n",
    "                                        download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(root='data', train=False,\n",
    "                                       download=True, transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "\n",
    "print('Data are ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have downloaded the data, now to extract the images and labels we will type the following command:\n",
    "\n",
    "```bash\n",
    "X = train_data.data\n",
    "y = train_data.targets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label ==> 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n",
       "          18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "         253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n",
       "         253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n",
       "         198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n",
       "          11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\n",
       "           2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\n",
       "          70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\n",
       "         225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\n",
       "         240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "         229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\n",
       "         253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\n",
       "         253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\n",
       "          80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examples\n",
    "# Images are in uint8\n",
    "print(f'The label ==> {train_data.targets[0]}')\n",
    "train_data.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to prepare the images here. The images are already in an appropriate format and require no special preparation other than normalization. As you may notice, the pixel values range from 0 to 255. This range can increase the computational load and introduce significant variance. Therefore, it is crucial to normalize the data. Using a MinMaxScaler normalization could be beneficial to scale our data from 0 to 1.\n",
    "\n",
    "The MinMax normalization formula is given by: $MinMax = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}$.\n",
    "\n",
    "Next, we will load these normalized data into a DataLoader, which will compact our data and organize it into batches. This will enable us to use the GPU for computations, making the process more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data ==> torch.Size([60000, 28, 28])\n",
      "test_data ==> torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Store the data\n",
    "# Print dimension of the data\n",
    "print(f'train_data ==> {train_data.data.shape}')\n",
    "print(f'test_data ==> {test_data.data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_grid_from_dataset(dataset, num_images=25, title=None):\n",
    "    \"\"\"\n",
    "    Plots a grid of images with labels directly from the dataset.\n",
    "\n",
    "    Args:\n",
    "    dataset (torch.utils.data.Dataset): Dataset from which to load the data.\n",
    "    num_images (int): Number of images to display in the grid.\n",
    "    title (str): Title of the plot.\n",
    "    \"\"\"\n",
    "    if len(dataset) < num_images:\n",
    "        raise ValueError(\"The dataset contains fewer images than the number requested for display.\")\n",
    "    \n",
    "    # Manually fetch a batch of images and labels\n",
    "    images = []\n",
    "    labels = []\n",
    "    for i in range(num_images):\n",
    "        image, label = dataset[i]\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "    \n",
    "    # Convert lists to tensor\n",
    "    images = torch.stack(images)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    # Create a grid of images\n",
    "    img_grid = torchvision.utils.make_grid(images, normalize=True, scale_each=True)\n",
    "\n",
    "    # Show images\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(img_grid.permute(1, 2, 0), cmap='gray', vmin=0, vmax=1)  # Rearrange the order of channels\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Adding the title\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "\n",
    "    # Show labels in the title or a legend (optional)\n",
    "    labels_str = ', '.join(map(str, labels.tolist()))\n",
    "    plt.xlabel(f\"Labels: {labels_str}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxoAAADaCAYAAAAhQDR7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAb+klEQVR4nO3dZ5Qc1bU24BqCEcESIKLBBItoMpgcTfQiB5OTyDkYk8FckgADsi1EFCBkAbZggYjGBgwih0VeRBtkkogi52T0/fjW3ezqq5Z6pJrpmenn+fW2uqr6aKbVM0dn1z5t48aNG1cAAABUaIpmDwAAAOh5TDQAAIDKmWgAAACVM9EAAAAqZ6IBAABUzkQDAAConIkGAABQORMNAACgclM1emBbW1tHjgMAAOgmGtnz24oGAABQORMNAACgciYaAABA5Uw0AACAyploAAAAlTPRAAAAKmeiAQAAVM5EAwAAqJyJBgAAUDkTDQAAoHImGgAAQOVMNAAAgMqZaAAAAJUz0QAAACpnogEAAFTORAMAAKiciQYAAFA5Ew0AAKByUzV7ALDccstFPvDAAyPvsssukYcPHx558ODBpfMff/zxDhwdAHQvgwYNinzwwQeXnnvmmWcib7zxxpFfffXVjh8YLceKBgAAUDkTDQAAoHJt48aNG9fQgW1tHT2Wyk055ZSR+/Tp09A5uXRnuummi7zwwgtHPuCAAyKfffbZpfO33377yF999VXkM844I/JJJ53U0Fh6qqWXXrr0+M4774zcu3fviZ7/8ccflx737du3knExadZZZ53IV155ZeQ111wz8r/+9a9OHVNPc/zxx0eu/fyYYoof/r9orbXWinz33Xd3+Lhgcv34xz+OPMMMM0TeaKONIs8222ylcwYOHBj566+/7sDRdS/zzTdf5MceeyzyjDPOWDou/9qXv8633nprh42tFSy00EKRp5566tJza6yxRuTzzz8/8vfffz9Zr3nDDTdE3m677SJ/8803k3XdRjUyhbCiAQAAVM5EAwAAqJyJBgAAULlu0952nnnmifyjH/0o8iqrrFI6brXVVouc6xK32mqryXr9MWPGRD7nnHMib7HFFqXjPv3008hPPfVU5Favl15hhRUiX3vttaXn8v0zud4vfy1zvWHtPRkrr7xy5FyX2lk1iu2VazWLovz3ue666zp7OJNt+eWXj/zoo482cSQ9S//+/SMfffTRkSdU09vgLXfQqeaff/7S4yOPPDJy/vxefPHFG7reHHPMEbm2dWsrGzt2bOR77rkn8qabbtqM4fRYiy22WOT8Ob311ltHzvfOFUVR/OQnP4mcP8Mn9zM7f28vvPDCyIceemjpuE8++WSyXmdyWNEAAAAqZ6IBAABUrkuXTi2zzDKR77jjjsiNtqqdXHl5K7eX/PzzzyP/5S9/KZ3z5ptvRv7www8jt0p7z9wSeNlll418xRVXRJ5zzjkbutaLL74Y+cwzz4w8YsSI0nH33Xdf5N/97neRTzvttIZep7PlFqRFURQLLrhg5O5QOlW7JJzLInKJY3dsid2VzDvvvJGnmWaaJo6ke1pxxRUj77zzzqXncvliLoPIDj/88Mj5c70oimL11VePfPnll0d++OGHJ22wPcQiiywSOZdu7LTTTqXjevXqFTl/Trz++uuRc+nsoosuWjp/m222iZxbhb7wwguTMOqeI/9uYpfvjnP66adH3nDDDZs4krJddtkl8qWXXlp67v777+/s4QQrGgAAQOVMNAAAgMp16dKpvPT3/vvvR57c0qm8vP3RRx+VnvvlL38ZOXctysvj1HfRRRdFzrukT4pcepV3jK3t4JVLkZZYYonJes3OkJc3i6IoHnzwwSaNZNLUlr7ttddekXOJXKuXMUyKddddN/JBBx003mNqv64bb7xx5HfeeadjBtZNbLvttpEHDRoUeZZZZikdl8t17rrrrsizzjpr5LPOOqvu6+Tz87Xzzrw9Wf4Z/Pvf/z5y/vrnHb8nJJfIbrDBBpFzd8nnn3++dE7+mtd+b1tZ7rS51FJLNW8gPdztt98euV7p1Lvvvlt6PHTo0Mj586Ne16ncja0oimLNNdds9zi7CisaAABA5Uw0AACAynXp0qkPPvgg8hFHHBE5lwo88cQTpXPyZnrZk08+GXm99daLnLs0FEW5A8khhxzSvgG3oOWWW670eKONNopcr+tQbenTzTffHDmXK7z11luR8/c5d/MqiqJYe+21J/qaXUlt16bu5pJLLqn7XC6DoDF5k9Fhw4ZFrlciWlvS04rdZaaa6ocfXXnDyIsvvjhy7oCXNy8riqI45ZRTIueudbm719VXXx15/fXXrzuWVtykMm9Uu+eee7b7/NGjR0fOP49z16ncjY/G5Pd87gA4IfnfTy7LbMXPlUZdcMEFka+//vrxHvPtt9+WHr/99tvteo3evXuXHj/zzDOR8+Z/WR5LV/pc6t6/8QAAAF2SiQYAAFA5Ew0AAKByXfoejSzXnt15552R8+6hRVFu6bbHHntEHjhwYOTa+zKyZ599NvLee+89SWPt6ZZeeunIuc1bUZTrCnPbtr///e+Ra9ve5rZteQf2fC/A2LFjIz/11FOl8/MO7vkekdwe9/HHHx/P36TzLLnkkpFnn332Jo5k8k2ovXTt+4GJ23XXXSPXtg7+X7kF6/Dhwzt6SF1e3mm63j1D+b2Y264WRVF88skn4z0nHzeh+zLGjBkT+c9//vOEB9sDbb311hM95pVXXon8yCOPlJ476qijIuf7MrK8yziNyTvY5/u9TjzxxLrn5Odyu/9zzz23wpH1LN99913keu/fyZVbPRdFUcw000wTPSd/Ln399deVj2lSWdEAAAAqZ6IBAABUrtuUTmX1lr2Loig+/vjj8f55bsE3YsSIyLnshvoWWmihyLnVcG0ZzXvvvRc5t6fN5QWfffZZ6Zy//e1v482TYtppp43829/+NvKOO+44WdedXHn30DzG7iKXe80///x1j3vjjTc6YzjdWu1Oxrvvvnvk/HmUyxgGDBjQ4ePqyk499dTS42OOOSZyLtE8//zzI+cyzAn9zMiOO+64ho47+OCDI+eyzlax1157Rc4lxrfddlvkl156KXLtLsmN6O4lps2WWzhPqHSKrmO77baLnP+NFUVjvzeccMIJlY+pClY0AACAyploAAAAleuWpVMTkpcI867VubPRuuuuGzkv9VKWd8k9++yzI+cyoNquX7vsskvkvDNlM8qFGt0ZtTMsvPDCdZ/Lnc66qvz9ry1p+Pe//x259v3A/zfffPNFvvbaaxs6Z/DgwZFzp71WkcsAcqlUURTFN998E/nWW2+NnLsZffnll3Wv3atXr8i5u1T+zGhra4tcW7p1ww03THDsPV3ubtRRZTkrr7xyh1y3FU0xRfn/lJWMN1cu5c6fbf369Ys89dRTN3StJ598MnLtbuRdhRUNAACgciYaAABA5Xpc6VTejC/ftZ83bLv44osjjxo1qnR+Lvc577zzIufOJq0ib3iXy6WyzTbbrPT47rvv7tAx9US1m1l1trzJ4q9+9avIeVO0CW1elrub5E5J/CB/XfPmjbXuuOOOyIMGDerQMXVFM844Y+T9998/cu3nby6X2nzzzSd63QUWWKD0+Morr4ycS2yza665JvKZZ5450degvtylqyiKYvrpp4+cS9Ty93mJJZaoe70HHngg8oMPPljFEHu02lKpVvx9pkq5FHbnnXeOnMvyJ2S11VaL3Oj3InfOO/rooyPfcsstkSdULtpMVjQAAIDKmWgAAACV63GlU9no0aMj9+/fP/Jll10WOS971T7Oy7vDhw+PnDei68kGDhwYOS9v5/KorlAqlTtq5CXiPOaubOaZZ27X8UsttVTpcf77r7POOpHnnnvuyD/60Y8i125emM/PS68PP/xw5K+//jryVFOVPzYee+yxhsfeSnJJzxlnnFH3uPvuuy/yrrvuGrne5qM9WX6f1m5smOVSnNlmmy3ybrvtFnnTTTeNvPjii5fOn2GGGSLn0oWcr7jiisi5JJey6aabLvJiiy0WOXcNq1d6WxT1P7+z2p+5+fv83//+t/HBwiTKpXy561xndbe89957Iw8ZMqRTXrMqVjQAAIDKmWgAAACVM9EAAAAq16Pv0ciuu+66yC+99FLkfB9CUZRr3E877bTI8847b+QBAwZEfuONNyodZ7NtvPHGkZdeeunIuXb5xhtv7MwhTVSu683jzDtmNlu+96G2nd2FF14Y+dhjj53otWrbo+Z7Ub777rvIX3zxReTnnnsu8tChQ0vn55bO+Z6bd955J/KYMWMi1+7y/sILL0x0zK1iUnYA/89//hM5f81bUd7xe+zYsZFnnXXW0nEvv/xy5EbaQ+adrIui3CpyzjnnjPzee+9FvummmxoYcWuo3aV4mWWWiZzf5/lrmT/zau+xyO1pc+vnfL9HNuWUU5Yeb7nllpFzG+j8/oGOkn/mTsq9oI3cl1Qr/26W73nK7W27KisaAABA5Uw0AACAyrVM6VT29NNPR95mm21Kz22yySaRcxvcffbZJ/KCCy4Yeb311uuIITZNLovJrSbffffdyFdddVWnjqkoimKaaaaJfOKJJ9Y97s4774ycd89strzL8auvvlp6bpVVVmnXtV577bXS49xqL5dIPfTQQ+26bq299947ci5dyaU+lB111FGRG10Sn1Dr21aTd5bP7YFvvvnm0nG5JXRuY57/LQwbNizyBx98UDp/xIgRkXO5T/7zVpc//3N5U1EUxciRI8d7zkknnRQ5fxbff//9pePy9y8fV9uG+H/Vls6dfvrpkfPn4fXXXx85t+RudblUpyjqfzatscYakc8999wOHVN3k39vXGuttSLvtNNOkW+99dbIX331VbtfY4899oh80EEHtfv8rsqKBgAAUDkTDQAAoHJt4xpp2VF0n12Wq1RvN+Tc2WeDDTYonXPXXXd1+Lg60tZbbx35r3/9a+TXX3898vzzz98pY8nlUscff3zkY445pnRc7vyVy33yMibtl0vk8vvirLPOKh2Xy4VaUe7Oljvw1NsxNpf3FEVR/PrXv+6QcfGDXBJSFOXuarmM5NBDD408ePDgDh9XV5O7S5188smRjzjiiLrn/OMf/4icy0hyGVxt6VPulLPssstGzl2jzjzzzMi1JVWbbbbZeMfyz3/+c7znf/jhh3XH/8QTT9R9rqeo3T29kV/7cnfDXJJLx+nTp0/k999/v+5xm266aeRmd51q5L1kRQMAAKiciQYAAFC5luw6lZcEa8sWll9++ci5XCrLy4j33HNPxaPrmjprk75chpKX67fddtvItaUnW221VYePix/kzi4UxW233RZ5pplmGu8xDz/8cOT+/ft39JCoUbvJZL1NPlux61TeDO+UU06JfPjhh0f+/PPPS+fk8tVcYpvLpfLP0toytLzh34svvhh5v/32izxq1KjIvXv3Lp2fO/XtuOOOkXNJSf53WasZpcDNlDeFLYpyF816chlyLimk49SW4vcUVjQAAIDKmWgAAACV69GlUwsvvHDkvPnJFltsEXmOOeZo6Fq5a8Nbb70VudFNubqL3F0s57x51iGHHFLpax522GGRc3ep3IHhyiuvjLzLLrtU+vowOfr27Ru53ufBeeedF/mzzz7r8DFRpgNdfblEJpdLffHFF5FrS21yWdJKK60Uebfddou84YYbRu7Vq1fp/NzRKm+Mm0uask8++aT0OHe6ynn77bePnEuqav3mN7+p+1xP9MILLzR7CN1C7rq2/vrrl57LG0t++eWXlb3m7rvvHvlPf/pTZdftSqxoAAAAlTPRAAAAKmeiAQAAVK7b36NRe4/FDjvsEPmAAw6IPN9887X72o8++mjkAQMGRO6sVq/NkFs95py/zuecc07koUOHls7Pu1nm2t2dd9458lJLLVU6Z+6554782muvRc511eeff35jfwE6RL5fZ8EFFyw99+CDD3b2cJoq15QXRVFMMcXE/7/mgQce6Kjh0ICe2jayCieccMJ4/zy3va3dGfzEE0+MvMACC0z0NfLxRVEUp59+euTaXasnR261m3Orq20vnO9Z7dev33jPyfdi1p4/evToCkfXXKuvvnrkY489NvJ6661XOi63Qa53L9GEzDzzzJHz/UsDBw6MPN1009U9P98XUuU9Ip3BigYAAFA5Ew0AAKBy3aZ0avbZZ4+82GKLRa5d0ltkkUXadd28Y29RFMVZZ50VOe9A3dPa2LZXXkbff//9I9fuyp3bENaW2NSTS29yC7l6S/p0vlxG10ipUE+Td6yvXVLPnw3ffPNN5NzS9p133um4wTFR9cpDKIq333478qyzzhp5mmmmiVxb7prdcsstke+5557I119/feRXXnmldE6V5VK037PPPhv5Zz/72XiPaZXfefLvkIsvvnjd44488sjIn376abtfJ//cWHbZZSPnn63ZXXfdVXp8wQUXRB41alS7X7+ZWu83BgAAoMOZaAAAAJXrcqVT+c78iy66KHIuXai31DchuetLvsu/dsfY7nY3f9VyGdMjjzwSefnllx/v8bVdv3KJW5a7UY0YMaL0XNU7jdOxVl555dLjYcOGNWcgnWjGGWeMXO89XhRF8cYbb0TOuyzTXPfee2/pcS7/a5USkXrWWGONyJtvvnnkXN7x7rvvls7J3QY//PDDyLl0kK5ryJAhkTfZZJMmjqT72G+//Trkuvnf1k033RS59veir776qkNevzNY0QAAACpnogEAAFSuKaVTK664YuTajYBWWGGFyHPNNVe7r51LnwYNGhT5tNNOi/z555+3+7qtYsyYMZG33HLLyPvss0/k448/vqFr5a//hRdeGPnFF1+cnCHSBHnDPuhunn766dLj/BmUS3Fzd6qxY8d2/MC6gNxB5/LLLx9vpmd57rnnIj///PORF1100WYMp6l22223yAceeGDkXXfddbKuW7up4RdffBE5l3JefPHFkWs/p3oKKxoAAEDlTDQAAIDKtY2rt1tI7YEVlk6cccYZkWtLp+rJy3v5zvzajX/OPvvsyB999NEkjhDo379/5NxlJi/1FkW5rK6nyt3VrrrqqtJzq622WuSXX3458gILLNDxA2OS5Pf2JZdcEvnuu++OfNBBB0XOpSZAz5Q3qcyfEUVRFKeeemrkmWaaKXLemPL222+PnDd8Loryxpg9SSNTCCsaAABA5Uw0AACAyploAAAAlWvKPRoA0Cy9e/eOfPXVV0ded911I48cOTJyboFZFFqkAxSFezQAAIAmMdEAAAAqp3QKgJaVy6gGDBgQeb/99ou85JJLls7R7hZA6RQAANAkJhoAAEDllE4BAADtonQKAABoChMNAACgciYaAABA5Uw0AACAyploAAAAlTPRAAAAKmeiAQAAVM5EAwAAqJyJBgAAUDkTDQAAoHImGgAAQOVMNAAAgMqZaAAAAJUz0QAAACpnogEAAFTORAMAAKiciQYAAFA5Ew0AAKByJhoAAEDlTDQAAIDKmWgAAACVM9EAAAAqN1WzBwAd5Y477ojc1tYWee21127GcAAAWooVDQAAoHImGgAAQOV6dOnU1FNPHXmVVVaJfNppp0VeddVVO3VMdKw//vGPkfP3fPjw4c0YDgBAy7KiAQAAVM5EAwAAqFyPLp3q06dP5FGjRkV+++23I88xxxylc/JzdA9nnHFG5H333Tfyt99+Gzl3oAIAoONZ0QAAACpnogEAAFSuR5dO1ZPLpZROdX8rrbRS5Nxp7L777ot89dVXd+qYAABanRUNAACgciYaAABA5VqydKqtra3ZQ2gJa6yxRuTjjjsu8vbbb1867oMPPmjXdWvPX3zxxSOPHj068uGHH96u6wIAUB0rGgAAQOVMNAAAgMqZaAAAAJVryXs0xo0bF3naaadt4kh6tiFDhkRecMEFI//85z8vHZfb0DYi3+9RFEXRt2/fyHvttVfkp556ql3XBQCgOlY0AACAyploAAAAlWvJ0qlsueWWKz1+8MEHmzSSnueLL76InMvVevXq1e5rLb300pHnmWee0nPff//9ZF0bAIDqWdEAAAAqZ6IBAABUrkeXTn333XeRP/7448h9+vSJ3K9fv04dU093yimnRF5iiSUiv/DCC5Eb7QY1/fTTRz7qqKMiTzfddKXjHnroocjXXHNN44MFAKDDWNEAAAAqZ6IBAABUrkeXTn300UeR77333sgbb7xxE0bTM/30pz8tPc4b5uXStQMOOCDy2LFjG7r2H/7wh8hbb7115DfffLN03KqrrtrYYAEA6DRWNAAAgMqZaAAAAJXr0aVTdIzcTWrkyJGl52aZZZbIgwcPjnz33Xc3dO3DDz88cv/+/cd7zIABAxq6FgAAzWNFAwAAqJyJBgAAUDkTDQAAoHItf49G3759mz2ELmuqqX54e+y0006RL7300shTTFGeq37//feRV1555cjHHnts5IEDB5bOmXnmmSPnNrZtbW2Rhw8fHvmiiy5q7C8AAEDTWNEAAAAqZ6IBAABUrm3cuHHjGjowlbF0RzfeeGPkvDP4J598Ujpuxhln7KwhdXm5XGrYsGHjPab2ffHSSy9F7tev33jPefTRR0uP55prrshzzjln5LyDeP5zAACaq5EphBUNAACgciYaAABA5Vqm69SoUaMi59IpyrbddtvIl112WeRvv/028kcffRR5hx12KJ3/4YcfRs7dpdZcc83Iv/jFL0rn5PKrvAyXdxl//fXXI6+11lql80ePHv1//yIAADSVFQ0AAKByJhoAAEDlWqZ06rXXXhvvn0899dSlx/POO2/kV199tUPH1BXts88+kfPXbMCAAZGHDh3a0LUOOuigyEOGDIm80korNXR+LqnKpW9KpQAAuj4rGgAAQOVMNAAAgMq1TOnUd999N94/r91wbppppumM4XRZN9xwQ+SRI0dGzl2fGpW7Ri222GJ1j9t+++0jP/PMM+M9ZsyYMe1+fQAAmseKBgAAUDkTDQAAoHImGgAAQOXaxuWtmCd0YM29DN3Zc889F3mRRRYpPXfhhRdG3n///TttTD1Fnz59IueWuPvtt1/k2va0Cy20UMcPDACAyjQyhbCiAQAAVM5EAwAAqFzLtLfNbrvttshzzTVX6bnDDjuss4fTo+Rys3333Tfyu+++G3nttdfu1DEBAND5rGgAAACVM9EAAAAq15KlU1ntHfPffPNNk0bSPc0777ylx3vuuWfk/LUdMmRIZLt8AwD0fFY0AACAyploAAAAlWv50qnevXuXHm+++eaRR44c2cmj6X5uv/320uNcSnXFFVdE/p//+Z9OGxMAAM1nRQMAAKiciQYAAFA5Ew0AAKByLXmPxjbbbBP566+/Lj333HPPdfZwurVhw4aVHp988smRb7zxxk4eDQAAXYUVDQAAoHImGgAAQOXaxtVujV3vwLa2jh5LpxkxYkTkRRddtPTcpptuGvnVV1/ttDEBAEB30cgUwooGAABQORMNAACgci1ZOgUAAEw6pVMAAEBTmGgAAACVM9EAAAAqZ6IBAABUzkQDAAConIkGAABQORMNAACgciYaAABA5Uw0AACAyploAAAAlTPRAAAAKmeiAQAAVM5EAwAAqJyJBgAAUDkTDQAAoHJTNXrguHHjOnIcAABAD2JFAwAAqJyJBgAAUDkTDQAAoHImGgAAQOVMNAAAgMqZaAAAAJUz0QAAACpnogEAAFTORAMAAKjc/wMwxZdHhwAm+QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_images_grid_from_dataset(train_data, num_images=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results indicate that our dataset contains 60,000 images, each with a height and width of 28 pixels. The structure of these data in a three-dimensional tensor (observations, height, width) also reveals that the images are in black and white, using a single channel. For color images, the tensor structure would include a fourth dimension corresponding to the color channels.\n",
    "\n",
    "Let's move on to normalization:\n",
    "The pixel values range from 0 to 255. Since the minimum value \n",
    "𝑋_min\n",
    "X_min is zero, we can simplify the normalization by merely dividing each pixel by 255. This operation scales the pixel values to the range [0, 1], which is often required to optimize the performance of machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "train_data.data = train_data.data / 255.0\n",
    "test_data.data = test_data.data / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7c624c56a290>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(f'label => {train_data.targets[0].numpy()}')\n",
    "plt.axis('off')\n",
    "plt.imshow(train_data.data[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Injection\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Choose the device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will explore the design and implementation of neural network models, which are essentially assemblies of functions $f(x)$ working together to solve complex problems through the aggregation of their predictions. The origins of neural networks date back to the 1960s with the introduction of the perceptron by Frank Rosenblatt. This model is described by the equation $y = W \\cdot X + b$, where $W$ represents the weights, $X$ the inputs, and $b$ the bias. However, despite this innovation, the perceptron was limited to solving linear problems due to its fundamental structure.\n",
    "\n",
    "The advent of the 1980s and 1990s saw significant advancements in this field thanks to the work of Geoffrey Hinton and others, who developed the MultiLayer Perceptron (MLP). The MLP is a network composed of multiple layers of perceptrons, enabling it to capture complex nonlinearities and handle problems well beyond the capabilities of the simple perceptron. This enhanced ability makes the MLP particularly effective for a variety of applications ranging from image recognition to predictive modeling in financial and medical domains.\n",
    "\n",
    "We will now proceed to the practical implementation of an MLP. This model, structured in successive layers of neurons with nonlinear activations, allows for modeling complex relationships between inputs and desired outputs. Achieving this objective involves a deep understanding of the underlying principles of tensor calculus, gradients, and backpropagation—crucial elements for the effective training of neural networks.\n",
    "\n",
    "This exploration is set within a context of ongoing research where neural network models are constantly refined to improve their accuracy, efficiency, and applicability to an increasingly broad array of complex challenges across various scientific and engineering fields. Thus, the development of MLPs is not just a technical task but also a quest to extend the boundaries of what machines can learn to do from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Trainer(dataloader, validloader, model, criterion, optimizer, epochs=10):\n",
    "    \"\"\"\n",
    "    This function allows you to train a model using training data sets (dataloader) while evaluating its performance on validation data.\n",
    "    Using tqdm (taqadum) provides a visual progress bar to track training and assessment progress.\n",
    "    A separate progress bar is used for each training epoch, showing progress across training batches, as well as a separate progress bar for evaluation on validation data.\n",
    "    ---\n",
    "    We return two lists, loss_cache and valid_cache, which respectively contain the average training losses for each epoch and the evaluation results on the validation data,\n",
    "    such as average loss and accuracy.\n",
    "    \"\"\"\n",
    "    size = len(dataloader)\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    loss_cache = []\n",
    "    valid_cache = []\n",
    "    acc = []\n",
    "    loss_test = []\n",
    "    training_time = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "      total_loss = 0\n",
    "      start_time = time.time()\n",
    "\n",
    "      for batch, (X, y) in enumerate(tqdm(dataloader, desc='Progress')):\n",
    "        # compute prediction and loss\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        preds = model(X)\n",
    "        loss = criterion(preds, y)\n",
    "\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "      # Time calcul\n",
    "      end_time = time.time()\n",
    "      train_time = end_time - start_time\n",
    "      training_time.append(train_time)\n",
    "\n",
    "      # Calculate loss\n",
    "      avg_loss = total_loss / size\n",
    "      loss_cache.append(avg_loss)\n",
    "      print(f\"\\nEpoch [{i + 1}/{epochs}] - Avg Training Loss: {avg_loss:.6f}\")\n",
    "\n",
    "      # Evaluation on validation set\n",
    "      model.eval()\n",
    "      total_correct = 0\n",
    "      total_test_loss = 0\n",
    "\n",
    "      with torch.no_grad():\n",
    "        for X_val, y_val in validloader:\n",
    "          X_val = X_val.to(device)\n",
    "          y_val = y_val.to(device)\n",
    "          preds_val = model(X_val)\n",
    "          total_test_loss += criterion(preds_val, y_val).item()\n",
    "          total_correct += (preds_val.argmax(1) == y_val).type(torch.float).sum().item()\n",
    "\n",
    "\n",
    "      avg_test_loss = total_test_loss / len(validloader)\n",
    "      accuracy = total_correct / len(validloader.dataset)\n",
    "      acc.append(accuracy)\n",
    "      loss_test.append(avg_test_loss)\n",
    "      valid_cache.append({\"test_loss\": loss_test, \"accuracy\": acc})\n",
    "\n",
    "      # Print loss and accuracy for the valid loader\n",
    "      print(f\"Validation - Avg Loss: {avg_test_loss:.6f} | Accuracy: {accuracy * 100:.2f}%\\n\")\n",
    "\n",
    "    # Time for the training\n",
    "    total_time = sum(training_time)\n",
    "    print(\"Total training time for {} epochs: {:.2f} seconds\".format(epochs, total_time))\n",
    "\n",
    "    return loss_cache, valid_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a Perceptron\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(Perceptron, self).__init__()\n",
    "        # Ensure the input dimension is correctly provided\n",
    "        self.y = nn.Linear(input_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Flatten x inside the model if not already flattened\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.y(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron(\n",
      "  (y): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Assuming input dimensions for MNIST (28x28 images)\n",
    "input_dim = 28 * 28\n",
    "num_classes = 10\n",
    "\n",
    "Perceptron = Perceptron(input_dim, num_classes)\n",
    "print(Perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "lr = 0.001\n",
    "\n",
    "# Define hyperparameters\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(Perceptron.parameters(), lr = lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 1875/1875 [00:04<00:00, 414.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/10] - Avg Training Loss: 2.274715\n",
      "Validation - Avg Loss: 2.250776 | Accuracy: 23.77%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 1875/1875 [00:04<00:00, 435.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [2/10] - Avg Training Loss: 2.230123\n",
      "Validation - Avg Loss: 2.215319 | Accuracy: 28.16%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 1875/1875 [00:04<00:00, 412.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [3/10] - Avg Training Loss: 2.200301\n",
      "Validation - Avg Loss: 2.193168 | Accuracy: 32.34%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 1875/1875 [00:04<00:00, 440.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [4/10] - Avg Training Loss: 2.179284\n",
      "Validation - Avg Loss: 2.171745 | Accuracy: 40.62%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 1875/1875 [00:04<00:00, 450.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [5/10] - Avg Training Loss: 2.157935\n",
      "Validation - Avg Loss: 2.148861 | Accuracy: 44.37%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 1875/1875 [00:04<00:00, 422.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [6/10] - Avg Training Loss: 2.136497\n",
      "Validation - Avg Loss: 2.127378 | Accuracy: 45.53%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 1875/1875 [00:04<00:00, 433.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [7/10] - Avg Training Loss: 2.117449\n",
      "Validation - Avg Loss: 2.109631 | Accuracy: 46.03%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 1875/1875 [00:04<00:00, 427.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8/10] - Avg Training Loss: 2.101713\n",
      "Validation - Avg Loss: 2.095159 | Accuracy: 46.79%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 1875/1875 [00:04<00:00, 438.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [9/10] - Avg Training Loss: 2.088776\n",
      "Validation - Avg Loss: 2.083102 | Accuracy: 47.27%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 1875/1875 [00:04<00:00, 444.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [10/10] - Avg Training Loss: 2.077839\n",
      "Validation - Avg Loss: 2.072970 | Accuracy: 47.50%\n",
      "\n",
      "Total training time for 10 epochs: 43.45 seconds\n"
     ]
    }
   ],
   "source": [
    "loss_cahe, valid_cache = Trainer(train_loader, test_loader, Perceptron, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.input = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.hl_1 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.out = nn.Linear(hidden_size, num_classes)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.input(x))\n",
    "        x = self.relu(self.hl_1(x))\n",
    "        x = self.out(x) \n",
    "        return F.softmax(x, dim=1) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
